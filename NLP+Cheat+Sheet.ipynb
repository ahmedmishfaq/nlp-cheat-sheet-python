{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Cheat Sheet - Python - Starter Kit - Nomenclature\n",
    "\n",
    "\n",
    "Demo: [![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/janlukasschroeder/nlp-cheat-sheet-python/master?filepath=NLP%2BCheat%2BSheet.ipynb)\n",
    "\n",
    "Stack\n",
    "- Python 3.6 (only! Run in venv if necessary)\n",
    "- spacy\n",
    "- lexnlp\n",
    "\n",
    "### Installation:\n",
    "spacy\n",
    "```shell\n",
    "pip install spacy\n",
    "python -m spacy download en \n",
    "# python -m spacy download en_core_web_lg\n",
    "```\n",
    "LexNLP ([installation guide here](https://github.com/LexPredict/lexpredict-contraxsuite/blob/master/documentation/Installation%20and%20Configuration/Installation%20and%20Configuration%20Guide.pdf))\n",
    "```shell\n",
    "pip install https://github.com/LexPredict/lexpredict-lexnlp/archive/master.zip\n",
    "python # to open REPL console\n",
    ">>> import nltk\n",
    ">>> nltk.download() # download all packages\n",
    "```\n",
    "\n",
    "#### Popular frameworks:\n",
    "- [GPT-2](https://openai.com/blog/better-language-models/) - generate fake news, text summaries\n",
    "- BERT\n",
    "- XLnet\n",
    "- [ERNIE](https://github.com/PaddlePaddle/ERNIE/)\n",
    "- [Holmes](https://github.com/msg-systems/holmes-extractor#the-basic-idea): document classification, search in documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets\n",
    "\n",
    "- [OntoNotes 5](https://github.com/ontonotes/conll-formatted-ontonotes-5.0) - corpus comprising various genres of text (news, conversational telephone speech, weblogs, usenet newsgroups, broadcast, talk shows) in three languages (English, Chinese, and Arabic) with structural information (syntax and predicate argument structure) and shallow semantics (word sense linked to an ontology and coreference).\n",
    "- [wiki_en_tfidf.mm in gensim](https://radimrehurek.com/gensim/wiki.html#latent-semantic-analysis) 3.9M documents, 100K features (distinct tokens) and 0.76G non-zero entries in the sparse TF-IDF matrix. The Wikipedia corpus contains about 2.24 billion tokens in total.\n",
    "- [GPT-2 Dataset](https://github.com/openai/gpt-2-output-dataset)\n",
    "- [All the news, Kaggle, 143K articles](https://www.kaggle.com/snapcrack/all-the-news)\n",
    "- [Daily news for stock market prediction](https://www.kaggle.com/aaron7sun/stocknews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer \n",
    "- Convert a collection of text documents to a matrix of token counts\n",
    "- [skikitLearn](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demos\n",
    "\n",
    "- [Stanford CoreNLP](http://corenlp.run/)\n",
    "- [Stanford NER](http://nlp.stanford.edu:8080/ner/process)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nltk (Python lib)\n",
    "\n",
    "- similar to spacy\n",
    "- supports more models and packages than spacy\n",
    "- simply GUI model download `nltk.download()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gensim (Python lib)\n",
    "\n",
    "- topic modelling for humans\n",
    "- loads a corpus\n",
    "- calcs similarities between query and indexed documents\n",
    "- SparseMatrixSimilarity\n",
    "- Latent Semantic Analysis\n",
    "\n",
    "https://radimrehurek.com/gensim/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lexnlp (Python lib)\n",
    "\n",
    "- Information retrieval and extraction for real, unstructured legal text\n",
    "- doesn't work; seemed to be excellent for legal + financial docs\n",
    "- docs aren't very good\n",
    "\n",
    "https://github.com/LexPredict/lexpredict-lexnlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word embeddings (=word vectors)\n",
    "\n",
    "- Word embeddings are vector representation of words. \n",
    "- Sentence: Word Embeddings are Word converted into numbers.\n",
    "- A word in this sentence may be “Embeddings” or “numbers ” etc.\n",
    "- A dictionary may be the list of all unique words in the sentence, eg [‘Word’,’Embeddings’,’are’,’Converted’,’into’,’numbers’]\n",
    "- A vector representation of a word may be a one-hot encoded vector where 1 stands for the position where the word exists and 0 everywhere else. \n",
    "\n",
    "### Example\n",
    "\n",
    "- numbers = [0,0,0,0,0,1] \n",
    "- converted = [0,0,0,1,0,0]\n",
    "\n",
    "** Either use pre-trained word vectors or train our own** \n",
    "\n",
    "#### Pre-trained word embeddings:\n",
    "- Word2Vec (Google, 2013), uses Skip Gram and CBOW\n",
    "- [On Google News trained (1.5GB)](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit) - vocabulary of 3 million words trained on around 100 billion words from the google news dataset\n",
    "- GloVe (Stanford)\n",
    "- [Stanford Named Entity Recognizer (NER)](https://nlp.stanford.edu/software/CRF-NER.shtml)\n",
    "- [LexPredict: pre-trained word embedding models for legal or regulatory text](https://github.com/LexPredict/lexpredict-lexnlp)\n",
    "- [LexNLP legal models](https://github.com/LexPredict/lexpredict-legal-dictionary) - US GAAP, finaical common terms, US federal regulators, common law\n",
    "\n",
    "#### Create word vectors yourself\n",
    "\n",
    "```python\n",
    "import gensim\n",
    "word2vev_model = gensim.models.word2vec.Word2Vec(sentence_list)\n",
    "```\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Frequency based word embeddings\n",
    "\n",
    "### Count Vector (= Document Term Matrix)\n",
    "\n",
    "![img](https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/06/04164920/count-vector.png)\n",
    "\n",
    "### TF-IDF\n",
    "\n",
    "Term Frequency - Inverse Document Frequency\n",
    "\n",
    "### Co-Occurrence Vector\n",
    "\n",
    "\n",
    "## 2. Prediction based word embeddings\n",
    "\n",
    "Using Neural Networks\n",
    "\n",
    "### CBOW (Continuous Bag of words)\n",
    "\n",
    "![cbow](https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/06/04205949/cbow1.png)\n",
    "\n",
    "### Skip Gram\n",
    "\n",
    "Skip – gram follows the same topology as of CBOW. It just flips CBOW’s architecture on its head. The aim of skip-gram is to predict the context given a word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine Similarity\n",
    "\n",
    "Compute cosine similarity between samples in X and Y.\n",
    "\n",
    "\n",
    "## Linear Kernel\n",
    "\n",
    "Compute the linear kernel between X and Y.\n",
    "\n",
    "\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.linear_kernel.html#sklearn.metrics.pairwise.linear_kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# John likes to watch movies. Mary likes movies too.\n",
    "BoW1 = {\"John\":1,\"likes\":2,\"to\":1,\"watch\":1,\"movies\":2,\"Mary\":1,\"too\":1};"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dataset\n",
    "nlp = spacy.load(\"en\")\n",
    "# Import large dataset. Needs to be downloaded first.\n",
    "# nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "\n",
    "Segmenting text into words, punctuation etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Larry', 'Page', 'founded', 'Google', 'in', 'early', '1990', '.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(\"Larry Page founded Google in early 1990.\")\n",
    "[token.text for token in doc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BILUO tagging\n",
    "\n",
    "ToDo: get list of labels, eg I, B, O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Larry, 'B', 'PERSON'),\n",
       " (Page, 'I', 'PERSON'),\n",
       " (founded, 'O', ''),\n",
       " (Google, 'B', 'ORG'),\n",
       " (in, 'O', ''),\n",
       " (early, 'B', 'DATE'),\n",
       " (1990, 'I', 'DATE'),\n",
       " (., 'O', '')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(token, token.ent_iob_, token.ent_type_) for token in doc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming\n",
    "\n",
    "ToDo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization\n",
    "\n",
    "Assigning the base forms of words, for example: \n",
    "- \"was\" → \"be\"\n",
    "- \"rats\" → \"rat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Was', 'be'),\n",
       " ('Google', 'Google'),\n",
       " ('founded', 'found'),\n",
       " ('in', 'in'),\n",
       " ('early', 'early'),\n",
       " ('1990', '1990'),\n",
       " ('?', '?')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(\"Was Google founded in early 1990?\")\n",
    "[(x.orth_, x.lemma_) for x in [token for token in doc]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spans\n",
    "Part of a given text. So doc[2:4] is a span starting at token 2, up to – but not including! – token 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'founded Google'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(\"Larry Page founded Google in early 1990.\")\n",
    "span = doc[2:4]\n",
    "span.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Detection\n",
    "\n",
    "Finding and segmenting individual sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Larry Page founded Google in early 1990.', 'Sergey Brin joined.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(\"Larry Page founded Google in early 1990. Sergey Brin joined.\")\n",
    "[sent.text for sent in doc.sents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-of-speech (POS) Tagging\n",
    "\n",
    "Assigning word types to tokens like verb or noun.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('We', 'PRON', 'pronoun'),\n",
       " ('are', 'VERB', 'verb'),\n",
       " ('reading', 'VERB', 'verb'),\n",
       " ('a', 'DET', 'determiner'),\n",
       " ('text', 'NOUN', 'noun'),\n",
       " ('.', 'PUNCT', 'punctuation')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(\"We are reading a text.\")\n",
    "[(x.orth_, x.pos_, spacy.explain(x.pos_)) for x in [token for token in doc]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('We', 'PRP', 'pronoun, personal'),\n",
       " ('are', 'VBP', 'verb, non-3rd person singular present'),\n",
       " ('reading', 'VBG', 'verb, gerund or present participle'),\n",
       " ('a', 'DT', 'determiner'),\n",
       " ('text', 'NN', 'noun, singular or mass'),\n",
       " ('.', '.', 'punctuation mark, sentence closer')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(x.orth_, x.tag_, spacy.explain(x.tag_)) for x in [token for token in doc]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependency Parsing\t\n",
    "\n",
    "Assigning syntactic dependency labels, describing the relations between individual tokens, like subject or object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('We', 'nsubj', 'nominal subject'),\n",
       " ('are', 'aux', 'auxiliary'),\n",
       " ('reading', 'ROOT', None),\n",
       " ('a', 'det', 'determiner'),\n",
       " ('text', 'dobj', 'direct object'),\n",
       " ('.', 'punct', 'punctuation')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(\"We are reading a text.\")\n",
    "# Dependency labels\n",
    "[(x.orth_, x.dep_, spacy.explain(x.dep_)) for x in [token for token in doc]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['reading', 'reading', 'reading', 'text', 'reading', 'reading']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Syntactic head token (governor)\n",
    "[token.head.text for token in doc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base noun phrases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'a red car']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(\"I have a red car\")\n",
    "[chunk.text for chunk in doc.noun_chunks]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition (NER)\n",
    "\n",
    "Labeling \"real-world\" objects, like persons, companies or locations.\n",
    "\n",
    "## Alternatives to spacy\n",
    "\n",
    "[LexNLP](https://lexpredict-lexnlp.readthedocs.io/en/latest/modules/extract/extract.html#pattern-based-extraction-methods) entities:\n",
    "- acts, e.g., “section 1 of the Advancing Hope Act, 1986”\n",
    "- amounts, e.g., “ten pounds” or “5.8 megawatts”\n",
    "- citations, e.g., “10 U.S. 100” or “1998 S. Ct. 1”\n",
    "- companies, e.g., “Lexpredict LLC”\n",
    "- conditions, e.g., “subject to …” or “unless and until …”\n",
    "- constraints, e.g., “no more than” or “\n",
    "- copyright, e.g., “(C) Copyright 2000 Acme”\n",
    "- courts, e.g., “Supreme Court of New York”\n",
    "- CUSIP, e.g., “392690QT3”\n",
    "- dates, e.g., “June 1, 2017” or “2018-01-01”\n",
    "- definitions, e.g., “Term shall mean …”\n",
    "- distances, e.g., “fifteen miles”\n",
    "- durations, e.g., “ten years” or “thirty days”\n",
    "- geographic and geopolitical entities, e.g., “New York” or “Norway”\n",
    "- money and currency usages, e.g., “$5” or “10 Euro”\n",
    "- percents and rates, e.g., “10%” or “50 bps”\n",
    "- PII, e.g., “212-212-2121” or “999-999-9999”\n",
    "- ratios, e.g.,” 3:1” or “four to three”\n",
    "- regulations, e.g., “32 CFR 170”\n",
    "- trademarks, e.g., “MyApp (TM)”\n",
    "- URLs, e.g., “http://acme.com/”\n",
    "\n",
    "Stanford NER entities:\n",
    "- Location, Person, Organization, Money, Percent, Date, Time\n",
    "\n",
    "NLTK\n",
    "- NLTK maximum entropy classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Larry Page', 'PERSON'),\n",
       " ('Google', 'ORG'),\n",
       " ('US', 'GPE'),\n",
       " ('early 1990', 'DATE')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(\"Larry Page founded Google in the US in early 1990.\")\n",
    "# Text and label of named entity span\n",
    "[(ent.text, ent.label_) for ent in doc.ents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lexnlp.extract.en as lexnlp\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10, 2.0]\n"
     ]
    }
   ],
   "source": [
    "text = \"There are ten cows in the 2 acre pasture.\"\n",
    "print(list(lexnlp.amounts.get_amounts(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'location_start': 5,\n",
       "  'location_end': 49,\n",
       "  'act_name': 'VERY Important Act',\n",
       "  'section': '12',\n",
       "  'year': '1954',\n",
       "  'ambiguous': False,\n",
       "  'value': 'section 12 of the VERY Important Act of 1954'}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import lexnlp.extract.en.acts\n",
    "text = \"test section 12 of the VERY Important Act of 1954.\"\n",
    "lexnlp.extract.en.acts.get_act_list(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification\n",
    "\n",
    "Assigning categories or labels to a whole document, or parts of a document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity\n",
    "How similar are two documents, sentences, token or spans?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.957709143352323"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc1 = nlp(\"I like cats\")\n",
    "doc2 = nlp(\"I like dogs\")\n",
    "# Compare 2 documents\n",
    "doc1.similarity(doc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.83117634"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \"cats\" vs \"dogs\"\n",
    "doc1[2].similarity(doc2[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.46475163"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \"I\" vs \"like dogs\"\n",
    "doc1[0].similarity(doc2[1:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.706799587675896"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(\"I like cats\")\n",
    "# L2 norm of \"I like cats\"\n",
    "doc.vector_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.933004"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# L2 norm of \"cats\"\n",
    "doc[2].vector_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.26763  ,  0.029846 , -0.3437   , -0.54409  , -0.49919  ,\n",
       "        0.15928  , -0.35278  , -0.2036   ,  0.23482  ,  1.5671   ,\n",
       "       -0.36458  , -0.028713 , -0.27053  ,  0.2504   , -0.18126  ,\n",
       "        0.13453  ,  0.25795  ,  0.93213  , -0.12841  , -0.18505  ,\n",
       "       -0.57597  ,  0.18538  , -0.19147  , -0.38465  ,  0.21656  ,\n",
       "       -0.4387   , -0.27846  , -0.41339  ,  0.37859  , -0.2199   ,\n",
       "       -0.25907  , -0.019796 , -0.31885  ,  0.12921  ,  0.22168  ,\n",
       "        0.32671  ,  0.46943  , -0.81922  , -0.20031  ,  0.013561 ,\n",
       "       -0.14663  ,  0.14438  ,  0.0098044, -0.15439  ,  0.21146  ,\n",
       "       -0.28409  , -0.4036   ,  0.45355  ,  0.12173  , -0.11516  ,\n",
       "       -0.12235  , -0.096467 , -0.26991  ,  0.028776 , -0.11307  ,\n",
       "        0.37219  , -0.054718 , -0.20297  , -0.23974  ,  0.86271  ,\n",
       "        0.25602  , -0.3064   ,  0.014714 , -0.086497 , -0.079054 ,\n",
       "       -0.33109  ,  0.54892  ,  0.20076  ,  0.28064  ,  0.037788 ,\n",
       "        0.0076729, -0.0050123, -0.11619  , -0.23804  ,  0.33027  ,\n",
       "        0.26034  , -0.20615  , -0.35744  ,  0.54125  , -0.3239   ,\n",
       "        0.093441 ,  0.17113  , -0.41533  ,  0.13702  , -0.21765  ,\n",
       "       -0.65442  ,  0.75733  ,  0.359    ,  0.62492  ,  0.019685 ,\n",
       "        0.21156  ,  0.28125  ,  0.22288  ,  0.026787 , -0.1019   ,\n",
       "        0.11178  ,  0.17202  , -0.20403  , -0.01767  , -0.34351  ,\n",
       "        0.11926  ,  0.73156  ,  0.11094  ,  0.12576  ,  0.64825  ,\n",
       "       -0.80004  ,  0.62074  , -0.38557  ,  0.015614 ,  0.2664   ,\n",
       "        0.18254  ,  0.11678  ,  0.58919  , -1.0639   , -0.29969  ,\n",
       "        0.14827  , -0.42925  , -0.090766 ,  0.12313  , -0.024253 ,\n",
       "       -0.21265  , -0.10331  ,  0.91988  , -1.4097   , -0.0542   ,\n",
       "       -0.071201 ,  0.66878  , -0.24651  , -0.46788  , -0.23991  ,\n",
       "       -0.14138  , -0.038911 , -0.48678  ,  0.22975  ,  0.36074  ,\n",
       "        0.13024  , -0.40091  ,  0.19673  ,  0.016017 ,  0.30575  ,\n",
       "       -2.1901   , -0.55468  ,  0.26955  ,  0.63815  ,  0.42724  ,\n",
       "       -0.070186 , -0.11196  ,  0.14079  , -0.022228 ,  0.070456 ,\n",
       "        0.17229  ,  0.099383 , -0.12258  , -0.23416  , -0.26525  ,\n",
       "       -0.088991 , -0.061554 ,  0.26582  , -0.53112  , -0.4106   ,\n",
       "        0.45211  , -0.39669  , -0.43746  , -0.6632   , -0.048135 ,\n",
       "        0.23171  , -0.37665  , -0.38261  , -0.29286  , -0.036613 ,\n",
       "        0.25354  ,  0.49775  ,  0.3359   , -0.11285  , -0.17228  ,\n",
       "        0.85991  , -0.34081  ,  0.27959  ,  0.03698  ,  0.61782  ,\n",
       "        0.23739  , -0.32049  , -0.073717 ,  0.015991 , -0.37395  ,\n",
       "       -0.4152   ,  0.049221 , -0.3137   ,  0.091128 , -0.38258  ,\n",
       "       -0.036783 ,  0.10902  , -0.38332  , -0.74754  ,  0.016473 ,\n",
       "        0.55256  , -0.29053  , -0.50617  ,  0.83599  , -0.31783  ,\n",
       "       -0.77465  , -0.0049272, -0.17103  , -0.38067  ,  0.44987  ,\n",
       "       -0.12497  ,  0.60263  , -0.12026  ,  0.37368  , -0.079952 ,\n",
       "       -0.15785  ,  0.37684  , -0.18679  ,  0.18855  , -0.4759   ,\n",
       "       -0.11708  ,  0.36999  ,  0.54134  ,  0.42752  ,  0.038618 ,\n",
       "        0.043483 ,  0.31435  , -0.24491  , -0.67818  , -0.33833  ,\n",
       "        0.039218 , -0.11964  ,  0.8474   ,  0.09451  ,  0.070523 ,\n",
       "       -0.2806   ,  0.296    , -0.17554  , -0.41087  ,  0.70748  ,\n",
       "        0.17686  ,  0.043479 , -0.31902  ,  0.64584  , -0.45268  ,\n",
       "       -0.7967   ,  0.099817 , -0.1734   ,  0.11404  , -0.36809  ,\n",
       "        0.12035  , -0.048582 ,  0.55945  , -0.51508  ,  0.072704 ,\n",
       "        0.18106  ,  0.07802  , -0.31526  ,  0.38189  ,  0.092801 ,\n",
       "       -0.044227 , -0.66154  , -0.020428 ,  0.059836 , -0.23628  ,\n",
       "       -0.017592 , -0.56481  , -0.52934  , -0.16392  ,  0.077331 ,\n",
       "        0.24583  , -0.32195  , -0.36811  , -0.037208 ,  0.26702  ,\n",
       "       -0.57907  ,  0.46457  , -0.54636  ,  0.11855  ,  0.092475 ,\n",
       "       -0.10469  ,  0.03319  ,  0.62616  , -0.33684  ,  0.045742 ,\n",
       "        0.25089  ,  0.28973  ,  0.060633 , -0.4096   ,  0.39198  ,\n",
       "        0.58276  ,  0.496    , -0.75881  ,  0.13655  ,  0.21704  ,\n",
       "       -0.37978  , -0.54051  , -0.22813  ,  0.28393  , -0.58739  ,\n",
       "        1.0472   , -0.13318  , -0.07325  ,  0.12991  , -0.44999  ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vector representation of \"cats\"\n",
    "doc[2].vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can also be done using sklearn's linear kernel (equivilant to cosine similarity)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unigram, bigrams, trigrams\n",
    "\n",
    "- Unigram = one word, eg the, and, of, hotel\n",
    "- Bigrams = two consecutive words, eg the hotel, in seattle, the city\n",
    "- Trigrams = three consecutive words, eg easy access to, high speed internet, the heart of\n",
    "\n",
    "Credits: https://towardsdatascience.com/building-a-content-based-recommender-system-for-hotels-in-seattle-d724f0a32070"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get all unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_top_n_words(corpus, n=None):\n",
    "    vec = CountVectorizer(stop_words='english').fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]\n",
    "common_words = get_top_n_words(df['desc'], 20)\n",
    "df2 = pd.DataFrame(common_words, columns = ['desc' , 'count'])\n",
    "df2.groupby('desc').sum()['count'].sort_values().iplot(kind='barh', yTitle='Count', linecolor='black', title='Top 20 words in hotel description after removing stop words')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get all bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_top_n_bigram(corpus, n=None):\n",
    "    vec = CountVectorizer(ngram_range=(2, 2), stop_words='english').fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]\n",
    "common_words = get_top_n_bigram(df['desc'], 20)\n",
    "df4 = pd.DataFrame(common_words, columns = ['desc' , 'count'])\n",
    "df4.groupby('desc').sum()['count'].sort_values(ascending=False).iplot(kind='bar', yTitle='Count', linecolor='black', title='Top 20 bigrams in hotel description After removing stop words')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get all trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_top_n_trigram(corpus, n=None):\n",
    "    vec = CountVectorizer(ngram_range=(3, 3), stop_words='english').fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]\n",
    "common_words = get_top_n_trigram(df['desc'], 20)\n",
    "df6 = pd.DataFrame(common_words, columns = ['desc' , 'count'])\n",
    "df6.groupby('desc').sum()['count'].sort_values(ascending=False).iplot(kind='bar', yTitle='Count', linecolor='black', title='Top 20 trigrams in hotel description after removing stop words')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"0bf72b5ccfc143b4871b353f00ec3470-0\" class=\"displacy\" width=\"750\" height=\"312.0\" direction=\"ltr\" style=\"max-width: none; height: 312.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">This</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">is</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">a</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">sentence</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0bf72b5ccfc143b4871b353f00ec3470-0-0\" stroke-width=\"2px\" d=\"M70,177.0 C70,89.5 220.0,89.5 220.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0bf72b5ccfc143b4871b353f00ec3470-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,179.0 L62,167.0 78,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0bf72b5ccfc143b4871b353f00ec3470-0-1\" stroke-width=\"2px\" d=\"M420,177.0 C420,89.5 570.0,89.5 570.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0bf72b5ccfc143b4871b353f00ec3470-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M420,179.0 L412,167.0 428,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0bf72b5ccfc143b4871b353f00ec3470-0-2\" stroke-width=\"2px\" d=\"M245,177.0 C245,2.0 575.0,2.0 575.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0bf72b5ccfc143b4871b353f00ec3470-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">attr</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M575.0,179.0 L583.0,167.0 567.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc = nlp(\"This is a sentence\")\n",
    "displacy.render(doc, style=\"dep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Larry Page\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " founded \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Google\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " in the \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    US\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " in \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    early 1990\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ".</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc = nlp(\"Larry Page founded Google in the US in early 1990.\")\n",
    "displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspired by: https://www.datacamp.com/community/blog/spacy-cheatsheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-cheat-sheet",
   "language": "python",
   "name": "nlp-cheat-sheet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
